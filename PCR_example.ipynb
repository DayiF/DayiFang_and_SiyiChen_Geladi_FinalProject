{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03219213  0.01492453  0.28226119 -0.95868115]\n",
      " [-0.92145084  0.38637527  0.00882484  0.03955522]\n",
      " [-0.38711382 -0.91994608 -0.05909019 -0.0187201 ]\n",
      " [-0.0059076  -0.06473493  0.95747537  0.28109677]]\n",
      "[[-1.01455946  2.37093862 -0.02738503  0.01241164]\n",
      " [-2.88891627 -0.51719545 -0.03634978  0.00765704]\n",
      " [ 0.90129992  1.04239594  0.15590092 -0.1337181 ]\n",
      " [ 0.32287412 -0.77497077  0.0544092   0.05001902]\n",
      " [ 4.24145574  0.17370851  0.06281221 -0.1798406 ]]\n",
      "[[ 0.65915556 -1.59922571  1.        ]\n",
      " [ 0.80641896  0.55457505  1.        ]\n",
      " [-0.11311905 -0.73543379  1.        ]\n",
      " [-0.18999328  0.47237091  1.        ]\n",
      " [-1.23079945 -0.41157448  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Principal Component Regression Example\n",
    "## Generating model with 2 factors and 4 explanatory variables\n",
    "## Try out partial correlation for dropping (or adding) factors (variable importance)\n",
    "###############\n",
    "## Apply algorithm for partial least squares as an alternative to PCR \n",
    "###############\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.tools import pca\n",
    "from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n",
    "\n",
    "\n",
    "# Example: principal component regression\n",
    "nobs = 1000\n",
    "f0 = np.c_[np.random.normal(size=(nobs,2)), np.ones((nobs,1))]\n",
    "f2xcoef = np.c_[np.repeat(np.eye(2),2,0),np.arange(4)[::-1]].T\n",
    "f2xcoef = np.array([[ 1.,  1.,  0.,  0.],\n",
    "                    [ 0.,  0.,  1.,  1.],\n",
    "                    [ 3.,  2.,  1.,  0.]])\n",
    "f2xcoef = np.array([[ 0.1,  3.,  1.,    0.],\n",
    "                    [ 0.,  0.,  1.5,   0.1],\n",
    "                    [ 3.,  2.,  1.,    0.]])\n",
    "x0 = np.dot(f0, f2xcoef)\n",
    "x0 += 0.1*np.random.normal(size=x0.shape)\n",
    "ytrue = np.dot(f0,[1., 1., 1.])\n",
    "y0 = ytrue + 0.1*np.random.normal(size=ytrue.shape)\n",
    "\n",
    "xred, fact, eva, eve  = pca(x0, keepdim=0)\n",
    "print(eve)\n",
    "print(fact[:5])\n",
    "print(f0[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS on original data\n",
      "[-0.00618973  0.11242093  0.65876826  0.09660511  0.13152815]\n",
      "-1433.74985251\n",
      "0.993185715098\n",
      "OLS on Factors\n",
      "[[  0.00000000e+00   3.54698430e+03   3.55189205e+03   1.11022302e-16\n",
      "    2.03217822e+03]\n",
      " [  1.00000000e+00   2.40839683e+03   2.41821234e+03   6.80048534e-01\n",
      "    6.50874761e+02]\n",
      " [  2.00000000e+00  -1.43515612e+03  -1.42043285e+03   9.93154313e-01\n",
      "    1.39372612e+01]\n",
      " [  3.00000000e+00  -1.43529438e+03  -1.41566336e+03   9.93162076e-01\n",
      "    1.39357565e+01]\n",
      " [  4.00000000e+00  -1.43374985e+03  -1.40921108e+03   9.93158321e-01\n",
      "    1.39579586e+01]]\n",
      "best result for k, by AIC, BIC, R2_adj, L1O\n",
      "[3 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "res = sm.OLS(y0, sm.add_constant(x0, prepend=False)).fit()\n",
    "print('OLS on original data')\n",
    "print(res.params)\n",
    "print(res.aic)\n",
    "print(res.rsquared)\n",
    "\n",
    "#print 'OLS on Factors'\n",
    "#for k in range(x0.shape[1]):\n",
    "#    xred, fact, eva, eve  = pca(x0, keepdim=k, normalize=1)\n",
    "#    fact_wconst = sm.add_constant(fact)\n",
    "#    res = sm.OLS(y0, fact_wconst).fit()\n",
    "#    print 'k =', k\n",
    "#    print res.params\n",
    "#    print 'aic:  ', res.aic\n",
    "#    print 'bic:  ', res.bic\n",
    "#    print 'llf:  ', res.llf\n",
    "#    print 'R2    ', res.rsquared\n",
    "#    print 'R2 adj', res.rsquared_adj\n",
    "\n",
    "print('OLS on Factors')\n",
    "results = []\n",
    "xred, fact, eva, eve  = pca(x0, keepdim=0, normalize=1)\n",
    "for k in range(0, x0.shape[1]+1):\n",
    "    #xred, fact, eva, eve  = pca(x0, keepdim=k, normalize=1)\n",
    "    # this is faster and same result\n",
    "    fact_wconst = sm.add_constant(fact[:,:k], prepend=False)\n",
    "    res = sm.OLS(y0, fact_wconst).fit()\n",
    "##    print 'k =', k\n",
    "##    print res.params\n",
    "##    print 'aic:  ', res.aic\n",
    "##    print 'bic:  ', res.bic\n",
    "##    print 'llf:  ', res.llf\n",
    "##    print 'R2    ', res.rsquared\n",
    "##    print 'R2 adj', res.rsquared_adj\n",
    "    prederr2 = 0.\n",
    "    for inidx, outidx in LeaveOneOut(len(y0)):\n",
    "        resl1o = sm.OLS(y0[inidx], fact_wconst[inidx,:]).fit()\n",
    "        #print data.endog[outidx], res.model.predict(data.exog[outidx,:]),\n",
    "        prederr2 += (y0[outidx] - resl1o.predict(fact_wconst[outidx,:]))**2.\n",
    "    results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n",
    "\n",
    "results = np.array(results)\n",
    "print(results)\n",
    "print('best result for k, by AIC, BIC, R2_adj, L1O')\n",
    "print(np.r_[(np.argmin(results[:,1:3],0), np.argmax(results[:,3],0),\n",
    "             np.argmin(results[:,-1],0))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA regression on simulated data,\n",
      "DGP: 2 factors and 4 explanatory variables\n",
      "==================================================\n",
      "  k       AIC        BIC       R2_adj      L1O    \n",
      "--------------------------------------------------\n",
      "     0   3546.984   3551.892      0.000   2032.178\n",
      "     1   2408.397   2418.212      0.680    650.875\n",
      "     2  -1435.156  -1420.433      0.993     13.937\n",
      "     3  -1435.294  -1415.663      0.993     13.936\n",
      "     4  -1433.750  -1409.211      0.993     13.958\n",
      "--------------------------------------------------\n",
      "Notes: k is number of components of PCA,\n",
      "       constant is added additionally\n",
      "       k=0 means regression on constant only\n",
      "       L1O: sum of squared prediction errors for leave-one-out\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.iolib.table import (SimpleTable, default_txt_fmt,\n",
    "                        default_latex_fmt, default_html_fmt)\n",
    "\n",
    "headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n",
    "numformat = ['%6d'] + ['%10.3f']*4 #'%10.4f'\n",
    "txt_fmt1 = dict(data_fmts = numformat)\n",
    "tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n",
    "\n",
    "print(\"PCA regression on simulated data,\")\n",
    "print(\"DGP: 2 factors and 4 explanatory variables\")\n",
    "print(tabl)\n",
    "print(\"Notes: k is number of components of PCA,\")\n",
    "print(\"       constant is added additionally\")\n",
    "print(\"       k=0 means regression on constant only\")\n",
    "print(\"       L1O: sum of squared prediction errors for leave-one-out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
