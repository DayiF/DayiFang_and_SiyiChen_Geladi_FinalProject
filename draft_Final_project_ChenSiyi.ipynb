{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Partial Least-Squares Regression in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy.linalg \n",
    "import re\n",
    "import sys, os\n",
    "import functools\n",
    "import glob\n",
    "numpy.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    " \n",
    "The partial least-squares regression method(PLS) is a statistical approach to make a linear regression model by projecting the predicted variables and the observable variables to a new space. By combining features of principal component analysis and multiple regression, PLS is more robust than principal component regression and simple OLS regression when we need a large set of variables with a few samples. \n",
    "In this final project, we implemented, tested and optimized the algorithm in the paper “PARTIAL LEAST SQUARES REGRESSION: A TUTORIAL”[1]. Then we offered a comparison of the code with other popular codes of PLS, followed by sevaral applications of the code in fields of chemistry and economy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Paper Background"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•\tState the research paper “PARTIAL LEAST SQUARES REGRESSION: A TUTORIAL”. \n",
    "\n",
    "•\tLiterature review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Literature review\n",
    "the review paper \"Overview and Recent Advances in Partial Least\n",
    "    Squares\" by Roman Rosipal and Nicole Kramer, LNCS, 2006, for\n",
    "    additional information related to the different variants of PLS and\n",
    "    for citations to more recent work related to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Algorithm Description"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•\tDescribe the concept of the algorithm and why it is interesting and/or useful. \n",
    "•\tDescribe the mathematical basis of the algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Applications"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•\tDescribe the importance of the methods in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLS is useful when you are dealing with data that have following characteristic:\n",
    " * make multidimensional predictions from multidimensional observations.\n",
    " * the dimensionality of the observation space is large.\n",
    " * the data you have available for constructing a prediction model is rather limited.  \n",
    "\n",
    "Compared with PLS, The more traditional multiple linear regression     (MLR) algorithms are likely to become numerically unstable under     these conditions(\"multicollinearity problem\" caused by strong correlations between the different predictor variables).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tImplementation "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implement the algorithm as a Python function or family of functions.\n",
    "•\tDoes it run?\n",
    "•\tIs it correct? How do you know? Use of tests\n",
    "•\tIs it written cleanly and efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multiple Linear Regression(MLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 MLR with only one dependent variable "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Consider MLR with only one dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 MLR with more than one dependent variable "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Consider MLR with more than one dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 When encounter the multicollinearity problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reducing the dimensionality N of the space of the predictor variables through the application of Principal Components Analysis(PCA) to your data.\n",
    "\n",
    "* using the method of Partial Least Squares (PLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PCA focuses solely on the space defined by the predictor variables and gives you a small orthogonal set of directions in that space that explain the most significant variations in just the space defined by x1, x2, ...., xN.Subsequently, you can use the principal directions (these will be linear combinations of the original predictor variables) for making predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Principal Component Regression(PCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Partial Least-Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLS also gives you a small set of principal directions (also called principal components) in the space defined  by the original predictor variables.  \n",
    "However, the directions yielded by PLS also take into account the observed variations in the space defined by the predicted variables y1, y2, ...., yM.  \n",
    "So if a principal component cannot explain the variations in the space defined by the predicted variables y1, y2, ..., yM, it will be ignored by PLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notations:\n",
    "\n",
    "* X:\n",
    "The matrix formed by the recorded values for the predictor variables x1, x2, ..., xN is typically denoted X.\n",
    "Each row of X is one observation record and each column of X stands\n",
    "for one predictor variable.  \n",
    "\n",
    "* Y:\n",
    "The values for the predicted variables y1, y2, ..., yM are also placed in a matrix that is typically denoted Y.  \n",
    "Each column of Y stands for one predicted variable and each row of Y for the values for all the predicted variables using the corresponding row of X for prediction.\n",
    "\n",
    "* So if we have I observation records available to us, X is a matrix of size IxN and Y is a matrix of size IxM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each principal component of X, usually referred to as a latent vector of X, is represented by t. \n",
    "The matrix whose columns are the latent vectors t is typically denoted T. \n",
    "If the p is the number of latent vectors discovered for X, then T is of size Ixp.  \n",
    "\n",
    "* Along the same lines, each latent vector of Y is typically denoted u and the matrix formed by these latent vectors is denoted U. \n",
    "In most variants of the PLS algorithm, the latent vectors for X and Y are discovered conjointly.\n",
    "In such cases, U will also be of size Ixp.\n",
    "\n",
    "\n",
    "* Loadings: Each latent vector t is a weighted linear combination of the columns of X and each latent vector u is a weighted linear combination of the columns of Y.  The weights that go into these\n",
    "    combinations can also be thought of as vectors. These weights are\n",
    "    referred to as loadings.\n",
    "\n",
    "\n",
    "* The loading vectors for X are represented\n",
    "    by p and those for Y by q.  The matrix formed by the p vectors\n",
    "    represented by P and by the q vectors by Q.\n",
    "\n",
    "\n",
    "* Two more matrices important to the discovery of the latent vectors\n",
    "    for X and Y are W and C.  As mentioned earlier, a latent vector t\n",
    "    is merely a weighted linear combination of the columns of X. For a\n",
    "    calculated t, we represent these weights in the form of a vector w,\n",
    "    and all the w vectors (for the different t vectors) taken together\n",
    "    constitute the matrix W.  \n",
    "\n",
    "\n",
    "\n",
    "* By the same token, a latent vector u is\n",
    "    is a weighted linear combination of the columns of Y and, for a\n",
    "    given u, we represent these weights in the form of a vector c.  All\n",
    "    the different vectors c (for the different u vectors) constitute\n",
    "    the matrix C.\n",
    "\n",
    "\n",
    "* A matrix of B: regression coefficients.  Once we have B, given a test data matrix Xtest, we can     make the prediction Ytest = (Xtest * B).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 The mathematical foundation of the algorithms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main steps for PLS  algorithm.  \n",
    "\n",
    "Goal: To find the latent vector t in the column space of X and the latent vector u in the column space of Y so that the covariance     (t' * u)/I is maximized.  \n",
    "\n",
    "* t' is the transpose of t;\n",
    "* I is the number of rows in X (and in Y)\n",
    "\n",
    "* Starting with a random guess for u, we iterate through the following eights steps until achieving the termination condition stated in the last step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (1) w = X' * u\n",
    "\n",
    "where X' is the transpose of X and u is the current value of the latent vector for Y.  \n",
    "\n",
    "We will use the elements of the vector w thus obtained as weights for creating a linear combination of the columns of X as a new candidate for t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step(2)  w =  w / ||w||\n",
    "\n",
    "where ||w|| is the norm of the vector w.  \n",
    "That is, ||w|| = sqrt(w' * w).  \n",
    "This step normalizes the magnitude of w to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (3)  t = X * w\n",
    "\n",
    "Now we have a new approximation to t.  For PLS, we also     normalize t as we normalized w in the previous step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (4)  c = Y' * t\n",
    "\n",
    "We awill use the elements of the vector c thus obtained as         weights for creating a linear combination of the columns           of Y for a new candidate as Y's latent vector u."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (5)  c = c / ||c||\n",
    "Normalize c just as we normalized w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (6)  u_old  =  u\n",
    "We store away the currently used value for u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (7)  u = Y * c\n",
    "Now we have a new candidate for the latent vector for Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step (8)  terminate the iterations if \n",
    "\n",
    "          ||u - u_old||<tol, where tol is a user-specified value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate p and q according to the obtained t and u\n",
    "\n",
    "Once we have a vector t from the column space of X and a vector u  from the column space of Y, we need to figure out what weighted  linear combination of the columns of X would result t and what  weighted linear combination of the column vectors of Y would result  in Y.  Referring to these weights by the vectors p and q, we calculate\n",
    "\n",
    "         p  =  (X' * t) / ||t||\n",
    "\n",
    "         q  =  (Y' * u) / ||u||\n",
    "\n",
    "\n",
    "* If this is our first pair (t,u) of latent vectors, we initialize the matrices T and U by setting them to the column vectors t and u,respectively.  \n",
    "\n",
    "* If this is not the first pair (t,u), we augment T and U by the additional column vectors t and u, respectively.  The same goes for the vectors p and q vis-a-vis the matrices P and Q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deflation:\n",
    "After the calculation of each pair (t,u) of latent vectors, it's\n",
    "time to subtract out from X the contribution made to it by t, and\n",
    "to subtract out from Y the contribution made to it by u. \n",
    "This step,called deflation as mentioned previously, is implemented as follows:\n",
    "\n",
    "For PLS( ):\n",
    "\n",
    "         X  =  X - (t * p')\n",
    "\n",
    "         Y  =  Y - b * t * c'\n",
    "\n",
    "         where b = t' * u. \n",
    "\n",
    "\n",
    "For PLS1(  ) and PLS2(  ):\n",
    "\n",
    "\n",
    "         X  =  X - (t * p')\n",
    "\n",
    "         Y  =  Y - (t * t' * Y) / ||t||\n",
    "\n",
    "\n",
    "This process of first finding the covariance maximizing latent  vectors t and u from the column spaces of X and Y, respectively,    and subsequently deflating X and Y as shown above is continued     until there is not much left of the matrices X and Y. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculate matrix B\n",
    "\n",
    "After the latent vectors of X and Y have been extracted, the last   step is the calculation of the matrix B of regression coefficients. For the PLS() implementation, this calculation involves\n",
    "\n",
    "                 _                  \n",
    "        B =  (P')  * diag[b1, b2, ...., bp] * C'\n",
    "\n",
    "          \n",
    "where inv((P')) is the pseudo-inverse of the transpose of P. The  second term above is the diagonal matrix formed by the b values     computed after the calculation of each pair (t,u) as mentioned     earlier. C' is the transpose of the C matrix.\n",
    "\n",
    "\n",
    "For PLS1() and PLS2(), the B matrix is calculated through the following formula:\n",
    "\n",
    "        B = W * (P' * W)^-1 * C'\n",
    "\n",
    "where (P' * W)^-1 is the matrix inverse of the product of the two  matrices P' and W.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions\n",
    "\n",
    "After you have calculated the B matrix, you are ready to make predictions in the future as new row vectors for the X matrix come along. Let's use the notation Xtest to denote the new data     consisting of the observed values for the predictor variables.\n",
    "Your prediction would now consist of\n",
    "\n",
    "                      Ytest =  Xtest * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.4.3 The code: PLS（ ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# centralize the data\n",
    "\n",
    "def centralize(X,Y):\n",
    "    X=X-np.mean(X)\n",
    "    Y=Y-np.mean(Y)\n",
    "    return (X,Y)\n",
    "\n",
    "def PLS(X,Y,tol=1e-6):\n",
    "\n",
    "    N = X.shape[0]\n",
    "    u = numpy.random.rand(1,N)\n",
    "    u = numpy.asmatrix(u).T\n",
    "    \n",
    "    #initialize T，U,W,C\n",
    "    T=U=W=C=None \n",
    "\n",
    "    #initialize P,Q,Bdiag\n",
    "    P=Q=Bdiag=None\n",
    "\n",
    "    i=0\n",
    "    while(True):                             \n",
    "        j = 0\n",
    "        while (True):\n",
    "            #Step(1)\n",
    "            w = X.T * u   \n",
    "            #Step(2) Normalizes the magnitude of w to 1.\n",
    "            w = w / numpy.linalg.norm(w)\n",
    "            #Step(3) a new approximation to t \n",
    "            t = X * w  \n",
    "            t = t / numpy.linalg.norm(t) #Normalize t\n",
    "            #Step(4) Obtain c\n",
    "            c = Y.T * t\n",
    "            #Step(5) Normalize c \n",
    "            c = c / numpy.linalg.norm(c)        \n",
    "            #Step(6) Store the currently used value for u\n",
    "            u_old = u\n",
    "            #Step(7) A new candidate for the latent vector for Y. \n",
    "            u = Y * c\n",
    "            #Step(8) Terminate the iterations if error is small enough\n",
    "            error = numpy.linalg.norm(u - u_old)\n",
    "            if error < tol:\n",
    "                break\n",
    "\n",
    "            j += 1 \n",
    "\n",
    "        #Obtain matrix b\n",
    "        b = t.T * u\n",
    "        b = b[0,0]\n",
    "        \n",
    "        if T is None:\n",
    "            T = t\n",
    "        else:\n",
    "            T = numpy.hstack((T,t))\n",
    "        if U is None:\n",
    "            U = u\n",
    "        else:\n",
    "            U = numpy.hstack((U,u))\n",
    "        if W is None:\n",
    "            W = w\n",
    "        else:\n",
    "            W = numpy.hstack((W,w))\n",
    "        if C is None:\n",
    "            C = c\n",
    "        else:\n",
    "            C = numpy.hstack((C,c))\n",
    "     \n",
    "        p = X.T * t / (np.linalg.norm(t) ** 2)\n",
    "        q = Y.T * u / (np.linalg.norm(u) ** 2)\n",
    "\n",
    "        if P is None:\n",
    "            P = p\n",
    "        else:\n",
    "            P = numpy.hstack((P,p))\n",
    "        if Q is None:\n",
    "            Q = q\n",
    "        else:\n",
    "            Q = numpy.hstack((Q,q))\n",
    "        if Bdiag is None:\n",
    "            Bdiag = [b]\n",
    "        else:\n",
    "            Bdiag.append(b)\n",
    "         \n",
    "        X_old = X\n",
    "        Y_old = Y\n",
    "        X = X - t * p.T\n",
    "        Y = Y - b * t * c.T\n",
    "        i += 1\n",
    "\n",
    "        if numpy.linalg.norm(X) < 0.001: break\n",
    "\n",
    "        #Obtain B\n",
    "        B = numpy.diag(Bdiag)\n",
    "        B = numpy.asmatrix(B) \n",
    "        B = numpy.linalg.pinv(P.T) * B * C.T\n",
    "        \n",
    "        #Prediction based on the original X:\n",
    "        \n",
    "        Y_predicted = centralize(X,Y)[0] * B\n",
    "        print(\"\\nY_predicted from the original X:\")\n",
    "        print(Y_predicted)\n",
    "\n",
    "        Y_predicted_with_mean = Y_predicted + np.mean(Y)\n",
    "        print(\"\\nThe predicted Y with the original Y's column-wise mean added:\")\n",
    "        print(Y_predicted_with_mean)\n",
    "         \n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Y_predicted from the original X:\n",
      "[[-0.124 -0.126]\n",
      " [-0.147 -0.15 ]\n",
      " [-0.143 -0.145]\n",
      " [-0.136 -0.139]]\n",
      "\n",
      "The predicted Y with the original Y's column-wise mean added:\n",
      "[[-0.297 -0.299]\n",
      " [-0.32  -0.322]\n",
      " [-0.315 -0.318]\n",
      " [-0.309 -0.311]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.646,  0.656],\n",
       "        [ 1.268,  1.288],\n",
       "        [ 1.076,  1.093]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "X = np.array([[0., 0., 1.],[1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n",
    "Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n",
    "\n",
    "#Testing\n",
    "PLS(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tOptimization\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•\tOpportunities for vectorization\n",
    "•\tOpportunities to use JIT or Cython\n",
    "•\tOpportunities to use multi-core machines\n",
    "•\tOpportunities to use Spark for distributed programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The different versions of the algorithm differ in the following ways: \n",
    "\n",
    "* (1) As each pair of latent\n",
    "    vectors, t from X and u from Y, is discovered, the different\n",
    "    versions of PLS differ with regard to the deflation step. Recall\n",
    "    that by deflation we mean how we subtract the influence of t from X\n",
    "    and of u of Y before starting search for the next pair (t,u) of\n",
    "    latent vectors. \n",
    "\n",
    "* (2) The different versions different with regard to\n",
    "    the normalization of the latent vectors. \n",
    "\n",
    "*  (3) the different\n",
    "    versions differ with regard to how the matrix B of the regression\n",
    "    coefficients is calculated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Three variants of the PLS algorithm: PLS( ),PLS1( ),PLS2( )\n",
    "\n",
    " \n",
    "\n",
    "*  PLS( ) Based on the description of the algorithm by Herve Abdi in\n",
    "    the article \"Partial Least Squares Regression and Projection on\n",
    "    Latent Structure Regression,\" Computational Statistics, 2010.  (This particular     version generates the best regression results.)  \n",
    "\n",
    "\n",
    "*  PLS1( ) and PLS2( ) are based on the description of the\n",
    "    algorithm in the previously cited paper by Roman Rosipal and Nicole Kramer.  The PLS1() algorithm has a special role amongst all of the     variants of the partial least squares method --- it is meant     specifically for the case when the matrix Y consists of only one     column vector.  That is, we use PLS1( ) when there is just one  predictor variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PLS1\n",
    "\n",
    "def PLS1(X,Y,tol=1e-6):\n",
    "    if Y.shape[1] != 1:\n",
    "        raise ValueError(\"PLS1 can only be called when the Y has only one column\")\n",
    "    X,Y=centralize(X,Y)\n",
    "    T=U=W=C=P=Q=B=t=w=u=c=p=q=None        \n",
    "    u = Y\n",
    "    i = 0\n",
    "    while (True):\n",
    "        w = np.dot(X.T , u)\n",
    "        w = w / numpy.linalg.norm(w)\n",
    "        t = np.dot(X , w)\n",
    "        c = np.dot(Y.T , t)\n",
    "        c = c / numpy.linalg.norm(c)        \n",
    "        u = np.dot(Y , c)\n",
    "        if T is None:\n",
    "            T = t\n",
    "        else:\n",
    "            T = numpy.hstack((T,t))\n",
    "        if U is None:\n",
    "            U = u\n",
    "        else:\n",
    "            U = numpy.hstack((U,u))\n",
    "        if W is None:\n",
    "            W = w\n",
    "        else:\n",
    "            W = numpy.hstack((W,w))\n",
    "        p = np.dot(X.T,t/(numpy.linalg.norm(t)**2))\n",
    "        q = np.dot(Y.T,u/(numpy.linalg.norm(u)**2))\n",
    "\n",
    "        if P is None:\n",
    "            P = p\n",
    "        else:\n",
    "            P = numpy.hstack((P,p))\n",
    "        if Q is None:\n",
    "            Q = q\n",
    "        else:\n",
    "            Q = numpy.hstack((Q,q))\n",
    "\n",
    "        X_old = X\n",
    "        Y_old = Y\n",
    "        xdot=np.dot(t, p.T)\n",
    "        X = X - xdot\n",
    "        Y = Y - np.dot(xdot,Y)/(np.linalg.norm(t)**2)\n",
    "        i += 1\n",
    "        if np.linalg.norm(X) < 0.001: break\n",
    "         \n",
    "        B = W * ((P.T * W).I) * T.T * Y\n",
    "       \n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Using C, C++ or Fortran give essentially identcial performance\n",
    "* Of the JIT solutions:\n",
    "    * Cython is the fastest but needs the extra work of type annotations\n",
    "    * numba is almost as fast and simplest to use - just say jit(functiion)\n",
    "    * numexpr is slightly slower and works best for small numpy expressions but is also very convenient\n",
    "* A pure numpy solution also perfroms reasonably and will be the shortest solutoin (a one-liner in this case)\n",
    "* The pure python approach is very slow, but serves as a useful template for converting to native langauge directly or via a JIT compiler\n",
    "* Note that the fsatest alternatives are approximately 1000 times faster than the pure python version for the test problem with n=1000 and p=3.\n",
    "\n",
    "### Recommendations for optimizing Python code\n",
    "\n",
    "* Does a reliable fast implementiaont already exist? If so, consider using that\n",
    "* Start with a numpy/python prototype - if this is fast enough, stop\n",
    "* See if better use of vectoriazaiton via numpy will help\n",
    "* Moving to native code:\n",
    "    * Most Python devleopers will use Cython as the tool of choice. Cython can also be used to access/wrap C/C++ code\n",
    "    * JIT compilation with numba is improving fast and may become competitive with Cython in the near future\n",
    "    * If the function is \"minimal\", it is usually worth considering numexpr because there is almost no work to be done\n",
    "    * Use C/C++/Fortran if you are fluent in those languages - you have seen how to call these functions from Python\n",
    "* If appropriate, consider parallelization (covered in later session)\n",
    "* As you optimize your code, remmeber:\n",
    "    * Check that is is giving correct results!\n",
    "    * Profile often - it is very hard to preidct the effect of an optimizaiton in general\n",
    "    * Remember that your time is precious - stop when fast enough\n",
    "    * If getting a bigger, faster machine will sovle the problem, that is sometimes the best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•\tApplication to simulated or toy datasets\n",
    "•\tApplication to one real data set\n",
    "•\tDiscussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tReproducible analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tReference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
